{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DUI DNN\n",
    "Deep Urban Interaction - Deep Neural Network  \n",
    "Interaction Classification with OpenCV, OpenPose, and PyTorch  \n",
    "Ryan Yan Zhang <ryanz@mit.edu>  \n",
    "City Science, MIT Media Lab  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "from pprint import pprint\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image from video with OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_file_name = 180305_ML Test Video_v1.mp4\n",
      "total_frames = 900.0\n",
      "180305_ML Test Video_v1_00000.png is saved\n",
      "180305_ML Test Video_v1_00150.png is saved\n",
      "180305_ML Test Video_v1_00300.png is saved\n",
      "180305_ML Test Video_v1_00450.png is saved\n",
      "180305_ML Test Video_v1_00600.png is saved\n",
      "180305_ML Test Video_v1_00750.png is saved\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# input parameters\n",
    "videos_src_path = '../data/viz/01_input_videos'\n",
    "images_save_path = '../data/viz/02_extracted_images'\n",
    "intervel_second = 5.0 #0.5    # extraction intervel in seconds\n",
    "\n",
    "\n",
    "#main code\n",
    "if not os.path.exists(images_save_path):\n",
    "    os.makedirs(images_save_path)\n",
    "    print('made path: {}'.format(images_save_path))\n",
    "# get file list\n",
    "video_file_names = [f for f in os.listdir(videos_src_path) if os.path.isfile(os.path.join(videos_src_path, f)) and f.endswith('.mp4')]\n",
    "\n",
    "for input_file_name in video_file_names:\n",
    "\n",
    "    print('input_file_name = ' + input_file_name)\n",
    "    vc = cv2.VideoCapture(os.path.join(videos_src_path, input_file_name))\n",
    "    fps = int(round(vc.get(cv2.CAP_PROP_FPS)))\n",
    "    intervel_frame = int(round(intervel_second * fps))\n",
    "    total_frames = vc.get(cv2.CAP_PROP_FRAME_COUNT) # maybe inaccurate\n",
    "    print('total_frames = ' + str(total_frames))\n",
    "\n",
    "    if (vc.isOpened() == False): \n",
    "        print('Error opening video stream or file')\n",
    "        exit()\n",
    "    \n",
    "    frame_get = 0\n",
    "    time_second = 0\n",
    "\n",
    "    while frame_get < total_frames:    # loop reading frames, set last frame\n",
    "        rval, frame = vc.read()\n",
    "        if rval == True:\n",
    "            if frame_get % intervel_frame == 0:    # save image every intervel_frame\n",
    "                time_second = int(frame_get/fps)    # int is floor\n",
    "                image_file_name = '{}_{}.png'.format(os.path.splitext(input_file_name)[0], str(frame_get).zfill(5))\n",
    "                cv2.imwrite(os.path.join(images_save_path, image_file_name), frame)    # save as image\n",
    "                #cv2.waitKey(0)\n",
    "                print('{} is saved'.format(image_file_name))\n",
    "            frame_get += 1\n",
    "\n",
    "    vc.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose json from image with OpenPose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made path: C:\\Users\\RYAN\\Documents\\GitHub\\CS_Urban_Interaction_CV\\interaction-analysis-python\\02_interaction-classification\\PyTorch_RZ\\data\\viz\\03_openpose_json\n",
      "openpose json time used: 6.152260780334473\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# input parameters\n",
    "input_path = r'C:\\Users\\RYAN\\Documents\\GitHub\\CS_Urban_Interaction_CV\\interaction-analysis-python\\02_interaction-classification\\PyTorch_RZ\\data\\viz\\02_extracted_images'\n",
    "output_path = r'C:\\Users\\RYAN\\Documents\\GitHub\\CS_Urban_Interaction_CV\\interaction-analysis-python\\02_interaction-classification\\PyTorch_RZ\\data\\viz\\03_openpose_jsons'\n",
    "openpose_path = r'C:\\OpenPose\\openpose-1.2.1-win64-binaries'\n",
    "\n",
    "# main code\n",
    "if not os.path.exists(input_path):\n",
    "    sys.exit('input path doesn\\'t exist!')\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    print('made path: {}'.format(output_path))\n",
    "\n",
    "start = time.time()\n",
    "cmd = r'cd {} && bin\\OpenPoseDemo.exe --image_dir \"{}\" --write_json \"{}\" --no_display --render_pose 0'.format(openpose_path, input_path, output_path)\n",
    "subprocess.check_output(cmd, shell=True)\n",
    "print(f'openpose json time used: {(time.time() - start)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## People Bounding box from OpenPose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180305_ML Test Video_v1_00000_tagged.png is saved\n",
      "180305_ML Test Video_v1_00150_tagged.png is saved\n",
      "180305_ML Test Video_v1_00300_tagged.png is saved\n",
      "180305_ML Test Video_v1_00450_tagged.png is saved\n",
      "180305_ML Test Video_v1_00600_tagged.png is saved\n",
      "180305_ML Test Video_v1_00750_tagged.png is saved\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "# constant parameters\n",
    "xx = 0\n",
    "yy = 1\n",
    "\n",
    "# input parameters\n",
    "img_path = r'C:\\Users\\RYAN\\Documents\\GitHub\\CS_Urban_Interaction_CV\\interaction-analysis-python\\02_interaction-classification\\PyTorch_RZ\\data\\viz\\02_extracted_images'\n",
    "json_path = r'C:\\Users\\RYAN\\Documents\\GitHub\\CS_Urban_Interaction_CV\\interaction-analysis-python\\02_interaction-classification\\PyTorch_RZ\\data\\viz\\03_openpose_jsons'\n",
    "img_out_path = r'C:\\Users\\RYAN\\Documents\\GitHub\\CS_Urban_Interaction_CV\\interaction-analysis-python\\02_interaction-classification\\PyTorch_RZ\\data\\viz\\04_bounding_box_images'\n",
    "json_label_path = r'C:\\Users\\RYAN\\Documents\\GitHub\\CS_Urban_Interaction_CV\\interaction-analysis-python\\02_interaction-classification\\PyTorch_RZ\\data\\viz\\05_bounding_box_jsons'\n",
    "fps = 30\n",
    "joint_confident_threshold = 0.1\n",
    "d_expand_fixed = 50.0\n",
    "\n",
    "# get file list. Make sure that the image names' order is corespondent to the json file names' order!!!\n",
    "img_file_names = [f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))]\n",
    "json_file_names = [f for f in os.listdir(json_path) if os.path.isfile(os.path.join(json_path, f))]\n",
    "\n",
    "\n",
    "# loop all the image files, json files, and save result\n",
    "for input_image_file_name in os.listdir(img_path):\n",
    "    if (input_image_file_name.endswith(\".png\") or input_image_file_name.endswith(\".jpg\")):\n",
    "        #print(os.path.join(img_path, input_image_file_name))\n",
    "        input_json_file_name = os.path.splitext(input_image_file_name)[0] + \"_keypoints.json\"\n",
    "        #print(os.path.join(json_path, input_json_file_name))\n",
    "        output_image_file_name = os.path.splitext(input_image_file_name)[0] + \"_tagged.png\"\n",
    "        #print(os.path.join(img_out_path, output_image_file_name))\n",
    "        json_label_file_name = input_image_file_name + \".json\"\n",
    "        \n",
    "        # Load an color image\n",
    "        img = cv2.imread(os.path.join(img_path, input_image_file_name))\n",
    "        # read the json file\n",
    "        with open(os.path.join(json_path, input_json_file_name)) as f:\n",
    "            json_data = json.load(f)\n",
    "        \n",
    "        # get data of effective joints of people in one json file\n",
    "        joint_confident_threshold = 0.1\n",
    "        # loop person in one json\n",
    "        people_list = []\n",
    "        for i in range(len(json_data[\"people\"])):\n",
    "            # loop joints of one person\n",
    "            effective_joint_xy_list = []\n",
    "            for j in range(int(len(json_data[\"people\"][i][\"pose_keypoints\"])/3)):\n",
    "                if json_data[\"people\"][i][\"pose_keypoints\"][j*3+2] > joint_confident_threshold:\n",
    "                    effective_joint_xy_list.append([json_data[\"people\"][i][\"pose_keypoints\"][j*3+xx], json_data[\"people\"][i][\"pose_keypoints\"][j*3+yy]])\n",
    "            people_list.append(effective_joint_xy_list)\n",
    "        \n",
    "        # analysis to extrapolate the bounding box\n",
    "        # simple max min bounding box of all the joints plus expand distance joint 0 to joint 1 for all 4 side\n",
    "        # loop person\n",
    "        box_list = []\n",
    "        for i in range(len(people_list)):\n",
    "            box = [[9999999,9999999],[0,0]] #[upper_left_xy,lower_right_xy]\n",
    "            joint_list = people_list[i]\n",
    "            d_expand = d_expand_fixed\n",
    "\n",
    "            # loop joint xy\n",
    "            for j in range(len(joint_list)):\n",
    "                joint_xy = joint_list[j]\n",
    "                box = [[min(box[0][xx],joint_xy[xx]),min(box[0][yy],joint_xy[yy])],[max(box[1][xx],joint_xy[xx]),max(box[1][yy],joint_xy[yy])]]\n",
    "            # expend the box by distance joint 0 to joint 1 for all 4 side\n",
    "            box = [[box[0][xx]-d_expand,box[0][yy]-d_expand],[box[1][xx]+d_expand,box[1][yy]+d_expand]]\n",
    "            box_list.append(box)\n",
    "        \n",
    "        # draw the boxes\n",
    "        for i in range(len(box_list)):\n",
    "            box = box_list[i]\n",
    "            cv2.rectangle(img,(int(box[0][xx]),int(box[0][yy])),(int(box[1][xx]),int(box[1][yy])),(255,255,255),2)\n",
    "        \n",
    "        # write the image in to file\n",
    "        cv2.imwrite(os.path.join(img_out_path, output_image_file_name), img)\n",
    "        print('{} is saved'.format(output_image_file_name))\n",
    "        \n",
    "        # construct json_label\n",
    "        data = {}\n",
    "        data['filename'] = input_image_file_name\n",
    "        data['file_ext'] = os.path.splitext(input_image_file_name)[1]\n",
    "        data['file_path'] = img_path\n",
    "        height, width, channels = img.shape\n",
    "        data['file_dim'] = [width, height]\n",
    "        data['people'] = []\n",
    "        for i in range(len(box_list)):\n",
    "            box = box_list[i]\n",
    "            pplData = {}\n",
    "            pplData['tl_coord'] = [int(box[0][xx]),int(box[1][yy])]\n",
    "            pplData['br_coord'] = [int(box[1][xx]),int(box[0][yy])]\n",
    "            pplData['dims'] = [int(box[1][xx]-box[0][xx]), int(box[1][yy]-box[0][yy])]\n",
    "            pplData['area'] = int(box[1][xx]-box[0][xx]) * int(box[1][yy]-box[0][yy])\n",
    "            data['people'].append(pplData)\n",
    "        data['count'] = len(box_list)\n",
    "        \n",
    "        with open(os.path.join(json_label_path, json_label_file_name), 'w') as outfile:\n",
    "            json.dump(data, outfile, indent=4)\n",
    "        \n",
    "        #t = t + 1  # run only first t-1 files\n",
    "        continue\n",
    "    else:\n",
    "        #t = t + 1  # run only first t-1 files\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Classification with PyTorch DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets.folder import ImageFolder, default_loader\n",
    "from torchvision import models\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training helpers\n",
    "def get_trainable(model_params):\n",
    "    return (p for p in model_params if p.requires_grad)\n",
    "\n",
    "\n",
    "def get_frozen(model_params):\n",
    "    return (p for p in model_params if not p.requires_grad)\n",
    "\n",
    "\n",
    "def all_trainable(model_params):\n",
    "    return all(p.requires_grad for p in model_params)\n",
    "\n",
    "\n",
    "def all_frozen(model_params):\n",
    "    return all(not p.requires_grad for p in model_params)\n",
    "\n",
    "\n",
    "def freeze_all(model_params):\n",
    "    for param in model_params:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation transforms\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "IMG_SIZE = 224  #224  #defined by NN model input\n",
    "_mean = [0.485, 0.456, 0.406]\n",
    "_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "train_trans = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE,IMG_SIZE)),  #256  #(IMG_SIZE, IMG_SIZE)  # some images are pretty small\n",
    "    #transforms.RandomCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(.3, .3, .3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(_mean, _std),\n",
    "])\n",
    "val_trans = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE,IMG_SIZE)),  #256  #(IMG_SIZE, IMG_SIZE)\n",
    "    #transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(_mean, _std),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set\n",
    "train_ds = ImageFolder(\"../data/raw/DUI/train\", transform=train_trans, loader=default_loader)\n",
    "val_ds = ImageFolder(\"../data/raw/DUI/valid\", transform=train_trans, loader=default_loader)\n",
    "#print(f'len(train_ds): {len(train_ds)}, len(val_ds): {len(val_ds)}')\n",
    "\n",
    "BATCH_SIZE = 128  #2  #256  #512  #32  #220 for resnet152 on Dell Presison 5520 laptop, 400 for resnet18\n",
    "\n",
    "n_classes = 2\n",
    "\n",
    "# DataLoader\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = models.resnet18(pretrained=True)\n",
    "#model = models.resnet50(pretrained=True)\n",
    "#model = models.resnet101(pretrained=True)\n",
    "#model = models.resnet152(pretrained=True)\n",
    "\n",
    "# Transfer learning or whole model training\n",
    "# Opt.1 Transfer learning\n",
    "'''\n",
    "# Freeze all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "#print(fall_frozen(model.parameters()): {all_frozen(model.parameters())}')\n",
    "\n",
    "model.fc = nn.Linear(512, n_classes)  # according to the model, 512 for resnet18, 2048 for resnet50 & resnet101 & resnet152\n",
    "\n",
    "model = model.to(device)\n",
    "'''\n",
    "\n",
    "# Opt.2 Whole model training\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    get_trainable(model.parameters()),\n",
    "    # model.fc.parameters(),\n",
    "    lr=0.001,\n",
    "    # momentum=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop\n",
    "if False:\n",
    "    N_EPOCHS = 10  #1  #2  #10\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        # start epoch\n",
    "        start_time = time.time()\n",
    "        start_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"Epoch {epoch+1}/{N_EPOCHS}\")\n",
    "        print(f'  Start Time: {start_datetime}')\n",
    "\n",
    "        # Train\n",
    "        model.train()  # IMPORTANT\n",
    "\n",
    "        running_loss, correct = 0.0, 0\n",
    "        for X, y in train_dl:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # with torch.set_grad_enabled(True):\n",
    "            y_ = model(X)\n",
    "            loss = criterion(y_, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Statistics\n",
    "            print(f\"    batch loss: {loss.item():0.3f}\")\n",
    "            _, y_label_ = torch.max(y_, 1)\n",
    "            correct += (y_label_ == y).sum().item()\n",
    "            running_loss += loss.item() * X.shape[0]\n",
    "\n",
    "        print(f\"  Train Loss: {running_loss / len(train_dl.dataset)}\")\n",
    "        print(f\"  Train Acc:  {correct / len(train_dl.dataset)}\")\n",
    "\n",
    "\n",
    "        # Eval\n",
    "        model.eval()  # IMPORTANT\n",
    "\n",
    "        running_loss, correct = 0.0, 0\n",
    "        with torch.no_grad():  # IMPORTANT\n",
    "            for X, y in val_dl:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "\n",
    "                y_ = model(X)\n",
    "\n",
    "                _, y_label_ = torch.max(y_, 1)\n",
    "                correct += (y_label_ == y).sum().item()\n",
    "\n",
    "                loss = criterion(y_, y)\n",
    "                running_loss += loss.item() * X.shape[0]\n",
    "\n",
    "        print(f\"  Valid Loss: {running_loss / len(val_dl.dataset)}\")\n",
    "        print(f\"  Valid Acc:  {correct / len(val_dl.dataset)}\")\n",
    "\n",
    "        # end epoch\n",
    "        end_time = time.time()\n",
    "        end_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        time_elapsed = end_time - start_time\n",
    "        datetime_elapsed = str(datetime.timedelta(seconds = time_elapsed))\n",
    "        print(f'  End Time: {end_datetime}')\n",
    "        print(f'  Time Elapsed: {datetime_elapsed}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with Trained Model\n",
    "\n",
    "\n",
    "# save the trained model weights\n",
    "model_weights_path = '../data/saved_model_weights/resnet18_whole'\n",
    "\n",
    "# save a trained model weights\n",
    "if False:\n",
    "    torch.save(model.state_dict(), model_weights_path)\n",
    "\n",
    "# load the trained model weights\n",
    "if True:\n",
    "    from torchvision import models\n",
    "    model = models.resnet18(pretrained=True)  # resnet50, 101, 152\n",
    "    model.load_state_dict(torch.load(model_weights_path))\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "# test data set\n",
    "test_ds = ImageFolder(\"../data/raw/DUI/test\", transform=val_trans, loader=default_loader)\n",
    "#print(f'len(test_ds) = {len(test_ds)}. ')\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "#print(f'test_ds[99]: \\n{test_ds[99]}')\n",
    "#print(f'test_ds[99][1]: \\n{test_ds[99][1]}')\n",
    "\n",
    "\n",
    "# predict\n",
    "model.eval()  # IMPORTANT\n",
    "with torch.no_grad():  # IMPORTANT\n",
    "    for X, y in test_dl:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        #print(f'y: \\t\\t\\t{y}')\n",
    "\n",
    "        y_ = model(X)\n",
    "        _, y_label_ = torch.max(y_, 1)\n",
    "        #print(f'y_label_: \\t\\t{y_label_}')\n",
    "        \n",
    "        is_correct = 'correct' if y_label_ == y else 'wrong'\n",
    "        #print(f'is_correct: \\t{is_correct}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
